import pandas as pd
import requests
import time
import os
import ast
import re
import math
from dotenv import load_dotenv
import logging
import json 
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
# åŠ è½½ .env (å¦‚æœä½ ä»ç„¶æƒ³ç”¨ç¯å¢ƒå˜é‡å­˜éƒ¨åˆ†key)
load_dotenv()

# 1. é…ç½®æ—¥å¿— (æ”¾åœ¨ä»£ç æœ€å¼€å¤´)
# è¿™æ ·é”™è¯¯ä¼šè¢«è®°å½•åˆ° 'api_errors.log' æ–‡ä»¶ä¸­ï¼Œä¸ä¼šå¼„ä¹±æ§åˆ¶å°
logging.basicConfig(
    filename='api_errors.log', 
    level=logging.ERROR, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8' # é˜²æ­¢ä¸­æ–‡ä¹±ç 
)
# ================= é…ç½®åŒºåŸŸ (æœ€é‡è¦éƒ¨åˆ†) =================
INPUT_FILE = 'input_text_long.csv'
PROMPT_FILE = 'prompt_template.txt'

# Excel æ ¼å¼é…ç½® 
ITEMS_PER_SHEET = 50
BASE_ROW_HEIGHT = 18
EMPTY_ROW_HEIGHT = 30
COL_WIDTHS = {
    "A": 80,  # Input (åŸå€¼150å¯èƒ½å¤ªå®½ï¼Œè®¾ä¸º80é…åˆè‡ªåŠ¨æ¢è¡Œæ›´ä½³ï¼Œå¯è‡ªè¡Œè°ƒæ•´)
    "B": 15,  # Model
    "C": 80,  # Infer
    "D": 10,  # æ‰“åˆ†
    "E": 50   # çº é”™
}

# --- åœ¨è¿™é‡Œå®šä¹‰ä½ çš„ 4-5 ä¸ªæ¨¡å‹é…ç½® ---
# æ ¼å¼ï¼š{"name": "æ˜¾ç¤ºåœ¨Excelçš„åå­—", "url": "APIåœ°å€", "key": "APIå¯†é’¥", "model_id": "ä¼ ç»™APIçš„æ¨¡å‹å‚æ•°å"}
MODELS_CONFIG = [
    {   "name": "gpt-5-chat-latest",
        "url": os.getenv("OPENAI_API_URL"), # ç¤ºä¾‹
        "key": os.getenv("OPENAI_API_KEY"), # å¯ä»¥ä»envè¯»ï¼Œä¹Ÿå¯ä»¥ç›´æ¥å†™å­—ç¬¦ä¸² "sk-xxxx"
        "model_id": os.getenv("OPENAI_MODEL_NAME4"),
        "params": {
            "temperature": 0.1,       # ç¿»è¯‘ä»»åŠ¡é€šå¸¸ç”¨ä½æ¸©åº¦
            "max_tokens": 500,        # é™åˆ¶è¾“å‡ºé•¿åº¦
            "top_p": 0.9
        }
    },
    {
        "name": "deepseek-chat",
        "url": os.getenv("OPENAI_API_URL_DEEPSEEK"), # ç¤ºä¾‹
        "key": os.getenv("OPENAI_API_KEY_DEEPSEEK"), 
        "model_id": os.getenv("OPENAI_MODEL_NAME_DEEPSEEK"),
        "params": {
            "temperature": 1,       # ç¿»è¯‘ä»»åŠ¡é€šå¸¸ç”¨ä½æ¸©åº¦
            "max_tokens": 500,        # é™åˆ¶è¾“å‡ºé•¿åº¦
            "top_p": 1
        }
    },
    {
        "name": "gemini-2.5-pro",
        "url": os.getenv("OPENRT_API_URL"), # å‡è®¾ä½ ç”¨çš„æ˜¯å…¼å®¹OpenAIæ ¼å¼çš„ä¸­è½¬
        "key": os.getenv("OPENRT_API_KEY"), 
        "model_id": os.getenv("OPENRT_MODEL_NAME_GEMINI"),
        "params": {
            "temperature": 0.2,       # ç¿»è¯‘ä»»åŠ¡é€šå¸¸ç”¨ä½æ¸©åº¦
            "max_tokens": 500,        # é™åˆ¶è¾“å‡ºé•¿åº¦
            "top_p": 1
        }
    },
    {
        "name": "claude",
        "url": os.getenv("OPENRT_API_URL"), # å‡è®¾ä½ ç”¨çš„æ˜¯å…¼å®¹OpenAIæ ¼å¼çš„ä¸­è½¬
        "key": os.getenv("OPENRT_API_KEY"), 
        "model_id": os.getenv("OPENRT_MODEL_NAME_CLAUDE"),
        "params": {
            "temperature": 0.7,       # ç¿»è¯‘ä»»åŠ¡é€šå¸¸ç”¨ä½æ¸©åº¦
            "max_tokens": 500,        # é™åˆ¶è¾“å‡ºé•¿åº¦
            "top_p": 1
        }
    },
    {
        "name": "qwen3-instruct",
        "url": os.getenv("OPENAI_API_URL_QWEN"), # å‡è®¾ä½ ç”¨çš„æ˜¯å…¼å®¹OpenAIæ ¼å¼çš„ä¸­è½¬
        "key": os.getenv("OPENAI_API_KEY_QWEN"), 
        "model_id": os.getenv("OPENAI_MODEL_NAME_QWEN"),
        "params": {
            "temperature": 0.7,       # ç¿»è¯‘ä»»åŠ¡é€šå¸¸ç”¨ä½æ¸©åº¦
            "max_tokens": 500,        # é™åˆ¶è¾“å‡ºé•¿åº¦
            "top_p": 0.8
        }
    },
    # ä½ å¯ä»¥ç»§ç»­æ·»åŠ æ›´å¤š...
]

# æ¨¡æ‹Ÿæ¨¡å¼ (True=ä¸èŠ±é’±æµ‹è¯•æµç¨‹, False=çœŸå®è¯·æ±‚)
MOCK_MODE = False 

# # ================= æ ¸å¿ƒå·¥å…·ï¼šæŒ‰æ ‡ç‚¹åˆ‡åˆ† =================
def split_text_by_punctuation(text):
    """
    å°†æ–‡æœ¬æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡åˆ†ï¼ŒåŒæ—¶ä¿ç•™æ ‡ç‚¹ç¬¦å·ã€‚
    ä¾‹å¦‚è¾“å…¥: "ä½ å¥½ï¼Œä¸–ç•Œã€‚" 
    è¾“å‡º: ['ä½ å¥½', 'ï¼Œ', 'ä¸–ç•Œ', 'ã€‚']
    """
    if not isinstance(text, str):
        text = str(text)
        
    # å®šä¹‰æ ‡ç‚¹ç¬¦å·çš„æ­£åˆ™æ¨¡å¼ (åŒ…å«å…¨è§’å’ŒåŠè§’å¸¸è§æ ‡ç‚¹)
    # è¿™é‡Œçš„ () æ˜¯æ•è·ç»„ï¼Œre.split ä¼šä¿ç•™æ•è·ç»„å†…çš„å†…å®¹ä½œä¸ºå•ç‹¬çš„åˆ—è¡¨é¡¹
    pattern = r'([ï¼Œ,ã€‚\.ï¼Ÿ\?ï¼!ï¼›;ï¼š:])'
    
    # åˆ‡åˆ†
    parts = re.split(pattern, text)
    
    # å»é™¤ç©ºå­—ç¬¦ä¸² (re.split æœ‰æ—¶ä¼šåœ¨é¦–å°¾äº§ç”Ÿç©ºä¸²)
    return [p for p in parts if p.strip() != '']

def is_punctuation(text):
    """åˆ¤æ–­ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦çº¯ç²¹æ˜¯æ ‡ç‚¹ç¬¦å·"""
    return re.match(r'^[ï¼Œ,ã€‚\.ï¼Ÿ\?ï¼!ï¼›;ï¼š:]+$', text.strip()) is not None

# ================= è¾…åŠ©å‡½æ•°ï¼šæ¸…æ´—æ•°æ® =================
def clean_gloss_text(text):
    """
    æ¸…æ´— gloss æ–‡æœ¬ã€‚
    ã€é‡è¦ä¿®æ”¹ã€‘ï¼šå› ä¸ºæˆ‘ä»¬åœ¨å¤–å±‚é€»è¾‘æ‰‹åŠ¨ä¿ç•™äº†åŸæ–‡æ ‡ç‚¹ï¼Œ
    è¿™é‡Œæˆ‘ä»¬è¦ã€å½»åº•åˆ é™¤ã€‘æ¨¡å‹å¯èƒ½ç”Ÿæˆçš„æ ‡ç‚¹ï¼Œé˜²æ­¢åŒé‡æ ‡ç‚¹ã€‚
    åªä¿ç•™ï¼šæ±‰å­—ã€è‹±æ–‡å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€‚
    """
    if pd.isna(text) or str(text).strip() == "":
        return ""
    
    text_str = str(text).strip()
    text_str = re.sub(r'^```(json)?|```$', '', text_str, flags=re.IGNORECASE | re.MULTILINE).strip()
    
    def extract_from_data(data):
        if isinstance(data, dict):
            for value in data.values():
                if isinstance(value, list):
                    return " ".join([str(x).strip() for x in value if str(x).strip()])
            for value in data.values():
                if isinstance(value, str):
                    return value.strip()
        elif isinstance(data, list):
            return " ".join([str(x).strip() for x in data if str(x).strip()])
        return None

    # å°è¯•è§£æ JSON/AST
    try:
        data = json.loads(text_str)
        res = extract_from_data(data)
        if res: text_str = res # å¦‚æœè§£ææˆåŠŸï¼Œæ›´æ–° text_str ä¸ºæå–å‡ºçš„å†…å®¹
    except: pass

    try:
        data = ast.literal_eval(text_str)
        res = extract_from_data(data)
        if res: text_str = res
    except: pass

    # æš´åŠ›æ¸…æ´—ï¼šå¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–å¼•å·å†…å®¹
    matches = re.findall(r'["\'](.*?)["\']', text_str)
    if matches:
        filtered = [m.strip() for m in matches if m.strip() not in ['gloss', 'json', '', ' ']]
        if filtered:
            text_str = " ".join(filtered)

    # --- æœ€ç»ˆæ¸…æ´— ---
    # æ›¿æ¢æ‰æ‰€æœ‰ éå•è¯å­—ç¬¦ (ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ä¸‹åˆ’çº¿) å’Œ ç©ºæ ¼
    # è¿™ä¸€æ­¥ä¼šæŠŠé€—å·ã€å¥å·ç­‰æ ‡ç‚¹å…¨éƒ¨åˆ æ‰ï¼Œç¡®ä¿åªç•™ä¸‹ Gloss è¯æ±‡
    cleaned = re.sub(r'[^\w\s\u4e00-\u9fa5]', ' ', text_str) 
    
    # åˆå¹¶å¤šä½™ç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

# ================= è¾…åŠ©å‡½æ•°ï¼šè¯»å– Prompt =================
def load_prompt():
    """
    ä» txt æ–‡ä»¶ä¸­è¯»å– system promptã€‚
    å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å› Noneã€‚
    """
    if not os.path.exists(PROMPT_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {PROMPT_FILE}")
        return None
    
    try:
        with open(PROMPT_FILE, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content:
                print(f"âš ï¸ è­¦å‘Š: {PROMPT_FILE} æ–‡ä»¶æ˜¯ç©ºçš„")
                return None
            return content
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å‡ºé”™: {e}")
        return None

# ================= é€šç”¨ API è°ƒç”¨å‡½æ•° =================
# è¿™æ˜¯ä¸€ä¸ªè£…é¥°å™¨ï¼Œæ„æ€æ˜¯ï¼š
# stop_after_attempt(3): æœ€å¤šè¯• 3 æ¬¡
# wait_fixed(2): æ¯æ¬¡å¤±è´¥åç­‰å¾… 2 ç§’
# è¿™æ ·ä½ å°±ä¸ç”¨åœ¨ä¸»ä»£ç é‡Œå†™å¤æ‚çš„ for å¾ªç¯äº†
# ä¿®æ”¹ä½ çš„è£…é¥°å™¨é…ç½®
@retry(
    # é‡åˆ°ä»»ä½•é”™è¯¯éƒ½é‡è¯•ï¼ˆä¹Ÿå¯ä»¥æŒ‡å®šåªåœ¨ RateLimitError æ—¶é‡è¯•ï¼‰
    reraise=True, 
    # æœ€å¤šé‡è¯• 5 æ¬¡
    stop=stop_after_attempt(5), 
    # æ ¸å¿ƒï¼šæŒ‡æ•°é€€é¿ã€‚
    # ç¬¬1æ¬¡å¤±è´¥ç­‰ 4ç§’ï¼Œç¬¬2æ¬¡ç­‰ 8ç§’ï¼Œç¬¬3æ¬¡ç­‰ 16ç§’... æœ€å¤§ç­‰ 60ç§’
    wait=wait_exponential(multiplier=1, min=4, max=60)
)
def call_translation_api_generic(text, system_prompt, config):
    # è·å–è¯¥æ¨¡å‹çš„è‡ªå®šä¹‰å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—å…¸
    custom_params = config.get('params', {})
    
    # æ‰“å°ä¸€ä¸‹å½“å‰ä½¿ç”¨çš„å‚æ•°ï¼ˆè°ƒè¯•ç”¨ï¼Œå¯æ³¨é‡Šæ‰ï¼‰
    # print(f"   [DEBUG] {config['name']} params: {custom_params}")

    if MOCK_MODE:
        return {"hksl": f"æ¨¡æ‹Ÿç»“æœ({config['name']} T={custom_params.get('temperature', 'default')}): {text[:5]}...", "status": "mock"}

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {config['key']}"
    }
    
    # 1. æ„å»ºåŸºç¡€ Payload
    payload = {
        "model": config['model_id'],
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text}
        ]
    }

    # 2. è®¾ç½®é»˜è®¤å€¼ (å¦‚æœ config é‡Œæ²¡å†™ï¼Œå°±ç”¨è¿™ä¸ªé»˜è®¤å€¼)
    # ç¿»è¯‘ä»»åŠ¡å»ºè®®é»˜è®¤ temperature è¾ƒä½
    if "temperature" not in custom_params:
        payload["temperature"] = 0.1 

    # 3. ã€å…³é”®æ­¥éª¤ã€‘å°† config é‡Œçš„ params åˆå¹¶è¿› payload
    # è¿™ä¼šè¦†ç›–ä¸Šé¢çš„é»˜è®¤å€¼ï¼Œå¹¶æ·»åŠ  max_tokens, top_p ç­‰å…¶ä»–å‚æ•°
    payload.update(custom_params)

    # å‘èµ·è¯·æ±‚
    response = requests.post(config['url'], json=payload, headers=headers, timeout=30)
    
    # å¦‚æœçŠ¶æ€ç æ˜¯ 4xx/5xxï¼Œè¿™é‡Œä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œè§¦å‘ @retry
    response.raise_for_status() 
    
    data = response.json()
    
    if "choices" in data:
        content = data['choices'][0]['message']['content']
        return {"hksl": content, "status": "success"}
    else:
        # è¿™ç§æ˜¯ API é€šäº†ä½†è¿”å›ç»“æ„ä¸å¯¹ï¼Œé€šå¸¸ä¸é‡è¯•ï¼Œç›´æ¥æŠ¥é”™
        return {"hksl": f"ç»“æ„é”™è¯¯: {data}", "status": "error"}

def save_formatted_excel(collected_data, filename):
    """
    å®ç°ä½ çš„æ ¼å¼åŒ–é€»è¾‘ï¼š
    1. åˆ† Sheet (æ¯50ä¸ªItemä¸€ä¸ªSheet)
    2. è®¾ç½®åˆ—å®½
    3. è®¾ç½®è¡Œé«˜ (æ™®é€šè¡Œ18ï¼Œåˆ†éš”ç©ºè¡Œ30)
    """
    print(f"æ­£åœ¨ç”Ÿæˆæ ¼å¼åŒ– Excel: {filename} ...")
    
    # å¿…é¡»ä½¿ç”¨ xlsxwriter å¼•æ“æ‰èƒ½è®¾ç½®æ ·å¼
    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
        workbook = writer.book
        # å®šä¹‰é€šç”¨æ ¼å¼ï¼šè‡ªåŠ¨æ¢è¡Œï¼Œé¡¶éƒ¨å¯¹é½
        fmt_wrap = workbook.add_format({'text_wrap': True, 'valign': 'top'})
        
        total_items = len(collected_data)
        num_sheets = math.ceil(total_items / ITEMS_PER_SHEET)
        
        for i in range(num_sheets):
            # 1. åˆ‡ç‰‡æ•°æ®
            start_idx = i * ITEMS_PER_SHEET
            end_idx = min((i + 1) * ITEMS_PER_SHEET, total_items)
            chunk = collected_data[start_idx:end_idx]
            
            sheet_name = f"Sheet{i+1}-{start_idx+1}_{end_idx}"
            
            # 2. å‡†å¤‡ DataFrame æ•°æ®å’Œè¡Œé«˜è®°å½•
            sheet_rows = []
            row_heights = {} # è®°å½•: è¡Œå·(ä»0å¼€å§‹) -> é«˜åº¦
            
            # ExcelWriter å†™å…¥æ—¶ï¼Œheader å ç¬¬0è¡Œï¼Œæ•°æ®ä»ç¬¬1è¡Œå¼€å§‹
            current_row_idx = 1 
            
            for item in chunk:
                # --- A. åŸæ–‡è¡Œ ---
                sheet_rows.append({
                    "Input": item['input'],
                    "Model": "", "Infer": "", "æ‰“åˆ†": "", "çº é”™": ""
                })
                row_heights[current_row_idx] = BASE_ROW_HEIGHT
                current_row_idx += 1
                
                # --- B. æ¨¡å‹ç»“æœè¡Œ ---
                for res in item['results']:
                    sheet_rows.append({
                        "Input": "",
                        "Model": res['model_name'],
                        "Infer": res['infer_text'],
                        "æ‰“åˆ†": "", "çº é”™": ""
                    })
                    row_heights[current_row_idx] = BASE_ROW_HEIGHT
                    current_row_idx += 1
                
                # --- C. ç©ºè¡Œåˆ†éš” ---
                sheet_rows.append({}) # ç©ºå­—å…¸äº§ç”Ÿç©ºè¡Œ
                row_heights[current_row_idx] = EMPTY_ROW_HEIGHT
                current_row_idx += 1
            
            # 3. å†™å…¥æ•°æ®åˆ° Sheet
            df_sheet = pd.DataFrame(sheet_rows)
            # ç¡®ä¿åˆ—é¡ºåº
            cols = ["Input", "Model", "Infer", "æ‰“åˆ†", "çº é”™"]
            # é˜²æ­¢ç©ºæ•°æ®æŠ¥é”™
            if not df_sheet.empty:
                df_sheet = df_sheet[cols]
            
            df_sheet.to_excel(writer, sheet_name=sheet_name, index=False)
            
            # 4. åº”ç”¨æ ¼å¼ (åˆ—å®½ & è¡Œé«˜)
            worksheet = writer.sheets[sheet_name]
            
            # è®¾ç½®åˆ—å®½
            worksheet.set_column('A:A', COL_WIDTHS['A'], fmt_wrap)
            worksheet.set_column('B:B', COL_WIDTHS['B'], fmt_wrap)
            worksheet.set_column('C:C', COL_WIDTHS['C'], fmt_wrap)
            worksheet.set_column('D:D', COL_WIDTHS['D'], fmt_wrap)
            worksheet.set_column('E:E', COL_WIDTHS['E'], fmt_wrap)
            
            # è®¾ç½®è¡Œé«˜
            for r_idx, height in row_heights.items():
                # set_row çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¡Œå· (0-indexed)
                # å› ä¸º header æ˜¯ç¬¬0è¡Œï¼Œæˆ‘ä»¬çš„ current_row_idx ä¹Ÿæ˜¯é…åˆ excel é€»è¾‘çš„
                worksheet.set_row(r_idx, height)
                
    print("âœ… Excel ç”Ÿæˆå®Œæ¯•ï¼")


# ================= ä¸»ç¨‹åºé€»è¾‘ =================
def main():
    system_prompt = load_prompt()
    if system_prompt is None:
        print("â›”ï¸ ç¨‹åºç»ˆæ­¢ï¼šå¿…é¡»æä¾› prompt æ¨¡æ¿æ–‡ä»¶ã€‚")
        return 

    if not os.path.exists(INPUT_FILE):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ {INPUT_FILE}")
        return

    try:
        df = pd.read_csv(INPUT_FILE)
    except:
        df = pd.read_csv(INPUT_FILE, encoding='gbk')

    target_column = 'input_text'
    if target_column not in df.columns:
        print(f"âŒ CSV ä¸­æ‰¾ä¸åˆ°åˆ—å '{target_column}'ã€‚ç°æœ‰åˆ—å: {list(df.columns)}")
        return
    # 2. Read the data / Slice the data
    rows = 50       
    start_row = 236 

    # æ–¹å¼ Aï¼šä½¿ç”¨ iloc
    df_subset = df.iloc[start_row : start_row + rows].copy()

    # --- 1. æ•°æ®æ”¶é›†é˜¶æ®µ ---
    collected_data = [] 

    print(f"âœ… å¼€å§‹å¤„ç† {len(df_subset)} æ¡æ•°æ® (åˆ†æ®µç¿»è¯‘æ¨¡å¼)...")
    print(f"ğŸ“ æ•°æ®èŒƒå›´: ç¬¬ {start_row} è¡Œ -> ç¬¬ {start_row + rows - 1} è¡Œ")

    # ã€ä¼˜åŒ–ç‚¹ 1ã€‘ä½¿ç”¨ enumerate è·å–å½“å‰å¾ªç¯çš„åºå·(i)ï¼Œindex ä»…ç”¨äºè®°å½•åŸå§‹è¡Œå·
    for i, (index, row) in enumerate(df_subset.iterrows()):
        source_text = row[target_column]
        if pd.isna(source_text) or str(source_text).strip() == "":
            continue

        # æ˜¾ç¤ºæ ¼å¼ï¼š[å½“å‰ç¬¬å‡ æ¡ / æ€»å…±å‡ æ¡] (åŸå§‹CSVè¡Œå·)
        print(f"\nProcessing [{i+1}/{len(df_subset)}] (Row {index}): {source_text[:15]}...")
        
        segments = split_text_by_punctuation(source_text)

        item_data = {
            "input": source_text,
            "results": []
        }

        for config in MODELS_CONFIG:
            print(f"  -> {config['name']}... ", end="", flush=True)

            final_parts = [] 
            sentence_has_error = False 

            for seg in segments:
                # ... (ä¸­é—´çš„ API è°ƒç”¨å’Œé”™è¯¯å¤„ç†é€»è¾‘ä¿æŒä¸å˜ï¼Œå†™å¾—å¾ˆå¥½) ...
                # ... (çœç•¥ä»¥èŠ‚çœç¯‡å¹…ï¼Œç›´æ¥ç”¨ä½ åŸæ¥çš„ä»£ç å³å¯) ...
                
                # è¿™é‡Œä¸ºäº†å®Œæ•´æ€§ï¼ŒæŠŠä½ çš„æ ¸å¿ƒé€»è¾‘æ”¾è¿™é‡Œå ä½
                if is_punctuation(seg) or not seg.strip():
                    final_parts.append(seg)
                    continue
                
                try:
                    res = call_translation_api_generic(str(seg), system_prompt, config)
                    if res['status'] == 'success' or res['status'] == 'mock':
                        final_parts.append(clean_gloss_text(res['hksl']))
                    else:
                        final_parts.append(f"[Logic Error: {res['hksl']}]")
                        sentence_has_error = True
                except Exception as e:
                    # å»ºè®®åœ¨æ–‡ä»¶å¤´éƒ¨ import tenacity
                    import tenacity 
                    real_error = e.last_attempt.exception() if isinstance(e, tenacity.RetryError) else e
                    final_parts.append(f"[API Fail: {str(real_error)}]")
                    sentence_has_error = True
                    logging.error(f"Error: {str(real_error)}")

            # 3. æ™ºèƒ½æ‹¼æ¥ç»“æœ (å¤„ç†ç©ºæ ¼)
            full_translation = ""
            for k, part in enumerate(final_parts):
                if is_punctuation(part):
                    full_translation += part
                else:
                    # é€»è¾‘ï¼šå½“å‰æ˜¯æ–‡æœ¬ã€‚
                    # å¦‚æœå‰ä¸€ä¸ªæ˜¯æ–‡æœ¬ï¼ŒåŠ ç©ºæ ¼ (WORD WORD)
                    # å¦‚æœå‰ä¸€ä¸ªæ˜¯æ ‡ç‚¹ï¼Œé€šå¸¸ä¹Ÿå»ºè®®åŠ ç©ºæ ¼ (WORD, WORD)ï¼Œé™¤éæ˜¯ä¸­æ–‡ç´§å‡‘æ’ç‰ˆ
                    # è¿™é‡Œä¿ç•™ä½ çš„é€»è¾‘ï¼Œä½†ä½ å¯ä»¥æ ¹æ®éœ€æ±‚æ”¹ä¸º:
                    # if k > 0 and final_parts[k-1].strip(): 
                    #    full_translation += " " + part
                    
                    # ä½ åŸæœ¬çš„é€»è¾‘ (Glossä¹‹é—´åŠ ç©ºæ ¼ï¼Œæ ‡ç‚¹åä¸åŠ ):
                    if k > 0 and not is_punctuation(final_parts[k-1]) and final_parts[k-1].strip():
                        full_translation += " " + part
                    else:
                        full_translation += part

            if sentence_has_error:
                print("âš ï¸ (Partial Error)") 
            else:
                print("âœ…") 

            item_data["results"].append({
                "model_name": config['name'],
                "infer_text": full_translation
            })
            
            if not MOCK_MODE: 
                time.sleep(0.2) 

        collected_data.append(item_data)

    # --- 2. Excel ç”Ÿæˆé˜¶æ®µ ---
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_filename = f"formatted_result_{timestamp}.xlsx"
    
    save_formatted_excel(collected_data, output_filename)
    # (å¯é€‰) å¼ºåˆ¶è®¾ç½®æŸä¸€åˆ—çš„æ ¼å¼ï¼Œç¡®ä¿ç”Ÿæ•ˆ
    # worksheet.set_column('C:C', COL_WIDTHS['C'], fmt_wrap) 
    # é€šå¸¸ä½ ç°åœ¨çš„ä»£ç æ˜¯å¯ä»¥å·¥ä½œçš„ï¼Œä½†å¦‚æœä¸è¡Œï¼Œéœ€è¦æ”¹ç”¨ workbook.add_format ç„¶ååœ¨ write æ—¶æŒ‡å®š

if __name__ == "__main__":
    main()